# -*- coding: utf-8 -*-
"""Test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/MScharnberg/IntSys19/blob/master/Test.ipynb

# Testing

Intelligent Systems

---

[@mats.scharnberg](mailto:mats.scharnberg@study.hs-duesseldorf.de)

[@christoph.schneider](mailto:christoph.schneider@study.hs-duesseldorf.de)

[@tobias.vossen](mailto:tobias.vossen@study.hs-duesseldorf.de)

## Contents

* Setup
* Data
* Model
* Deployment

## Setup

### Magic
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""### Requirements"""

import shutil

import numpy as np
print('NumPy version:', np.__version__)

from matplotlib import pyplot as plt

from tensorboard import version
print('TensorBoard version:', version.VERSION)
from tensorboard.plugins.hparams import api as hp

import tensorflow as tf
print('TensorFlow version:', tf.__version__)
from tensorflow import keras

import tensorflow_datasets as tfds
print('TensorFlow Datasets version:', tfds.__version__)

"""## Data

*   Load dataset
*   Explore dataset
*   Preprocess dataset
*   Visualize dataset

### Constants
"""

_BS = 32 #@param {type:"slider", min:16, max:64, step:16}
_DIM = 28 #@param ["28"] {type:"raw"}
_DS = 'mnist'
_INPUT = 196 # Noise input
_SHAPE = (_DIM, _DIM, 1) # Image shape
_SIZE = '[:100%]' # Dataset size

"""### Load dataset"""

def load_dataset(dataset):
  """Load dataset by means of TFDS (TensorFlow Datasets)
  
  Args:
    dataset: String
  
  Returns:
    train: tf.data.Dataset
    test: tf.data.Dataset
  """

  (_, test), info = tfds.load(dataset,
                            split=['train'+_SIZE, 'test'+_SIZE],
                            as_supervised=True,
                            with_info=True)

  print('Description:', info.description)
  print('Source:', info.homepage)
  print('Total number of test examples:', info.splits['test'].num_examples)
  return test

test_dataset = load_dataset(_DS)

"""### Preprocess dataset"""

def normalize(image, label):
  """Normalize dataset 
  
  Normalize:
    Cast -> Normalize

  Args:
    image: tf.Tensor as Integer
    label: tf.Tensor as Integer
  
  Returns:
    image: tf.Tensor as Float
    label: tf.Tensor as Float
    noise: tf.Tensor as Float
  """

  image = tf.cast(image, tf.float32)
  image = (image - 127.5) / 127.5 # -1...1
  
  label = tf.cast(label, tf.float32) # (1)
  # noise = tf.random.normal((_INPUT, )) # (196)
  noise = tf.random.uniform((_INPUT, ), minval=-1, maxval=1) # (196)
  label = tf.expand_dims(label, -1)

  return image, label, noise

def preprocess(dataset, shuffle=True, batch=True, prefetch=True):
  """Preprocess dataset

  Preprocess: 
    Normalize -> Shuffle -> Batch -> Prefetch
  
  Args:
    dataset: tf.data.Dataset
    shuffle: boolean
    batch: boolean
    prefetch: boolean
  
  Returns:
    dataset: tf.data.Dataset
  """

  dataset = dataset.map(normalize)
  if shuffle: dataset = dataset.shuffle(tf.data.experimental.cardinality(dataset))
  if batch: dataset = dataset.batch(_BS, drop_remainder=True)
  if prefetch: dataset = dataset.prefetch(1)
  return dataset

test_dataset = preprocess(test_dataset)

"""### Visualize dataset"""

def visualize(dataset):
  """Visualize dataset
  
  Args:
    dataset: tf.data.Dataset
  """

  dataset = dataset.unbatch()
  fig = plt.figure(figsize=(8, 8))
  i = 0
  for image, label, noise in dataset.take(16):
    fig.add_subplot(4, 4, i+1)
    plt.imshow(tf.squeeze(image), cmap='gray')
    plt.title(int(label.numpy()))
    plt.axis('off')
    i = i+1

  plt.suptitle('Real data instances')
  plt.show()

visualize(test_dataset)

"""## Model

*   Get model
*   Explore model
*   Evaluate model

### Parameters
"""

_INPUT = 196 # Generator input

"""### Get model"""

generator_path = keras.utils.get_file('generator.h5', 'https://github.com/MScharnberg/IntSys19/blob/master/model/generator.h5?raw=true', cache_subdir='model')
shutil.move(generator_path, './generator.h5')

discriminator_path = keras.utils.get_file('discriminator.h5', 'https://github.com/MScharnberg/IntSys19/blob/master/model/discriminator.h5?raw=true', cache_subdir='model')
shutil.move(discriminator_path, './discriminator.h5')

"""### Explore model"""

generator = keras.models.load_model('generator.h5')
generator.summary()

discriminator = keras.models.load_model('discriminator.h5')
discriminator.summary()

"""### Evaluate model"""

fig = plt.figure(figsize=(16, 32))

for i in range(10):

  # Copy
  noise = tf.random.normal((16, _INPUT, ))
  fake_images = generator(noise, training=False)
  fake_output = discriminator(fake_images, training=False)

  # Original
  real_images = tf.zeros((1, 28, 28, 1), dtype=float)
  for image, label, noise in test_dataset.unbatch().take(-1):
      if label == i:
        real_images = tf.concat([real_images, tf.expand_dims(image, 0)], 0)
        if len(real_images) >= 17: 
          break
          
  real_images = real_images[1:, :, :]
  real_output = discriminator(real_images, training=False)
  
  ax1 = fig.add_subplot(10, 3, 3*i + 1)
  ax1.set_axis_off()
  ax1.imshow(tf.squeeze(fake_images[0]), cmap='gray')

  ax2 = fig.add_subplot(10, 3, 3*i + 2)
  ax2.hist([tf.squeeze(fake_output), tf.squeeze(real_output)])
  ax2.axes.set_xticks([-1, 0, 1])
  ax2.axes.set_xticklabels(['Copy', 'Unsure', 'Original'])
  ax2.legend(['Fake', 'Real'])

  ax3 = fig.add_subplot(10, 3, 3*i + 3)
  ax3.set_axis_off()
  ax3.imshow(tf.squeeze(real_images[0], -1), cmap='gray')

plt.suptitle('Discriminator classification')
plt.show()

"""## Deployment

*   Use model

### Use model
"""

def generate_multiple():
  """Generate multiple digits

  Args:
    num: Integer
  """

  fig = plt.figure(figsize=(8, 8))

  for i in range(16):
    fig.add_subplot(4, 4, i+1)
    input_vector = tf.random.normal((1, _INPUT))
    image = generator(input_vector, training=False)
    plt.imshow(tf.squeeze(image), cmap='gray')
    plt.axis('off')

  plt.suptitle('Fake data instances')
  plt.show()

generate_multiple()