"""IntSys19

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/MScharnberg/IntSys19/blob/master/Notebook.ipynb

# Notebook

Intelligent Systems

---

[@mats.scharnberg](mailto:mats.scharnberg@study.hs-duesseldorf.de)

[@christoph.schneider](mailto:christoph.schneider@study.hs-duesseldorf.de)

[@tobias.vossen](mailto:tobias.vossen@study.hs-duesseldorf.de)

## Contents

* Setup
* Data
* Model
* Deployment

## Setup
"""

# Showcase this Notebook?
_SHOWCASE = True #@param ["True", "False"] {type:"raw"}

"""### Magic"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %load_ext tensorboard

"""### Libraries"""

import datetime
import math
import os
from timeit import default_timer as timer

import numpy as np
from matplotlib import pyplot as plt
from tensorboard import version
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds

print('Keras version:', keras.__version__)
print('NumPy version:', np.__version__)
print('TensorBoard version:', version.VERSION)
print('TensorFlow version:', tf.__version__)
print('TensorFlow Datasets version:', tfds.__version__)

"""### Utils"""

def check_env():
  """Check hardware accelerator for training"""

  if not tf.test.gpu_device_name():
    print('WARNING: Running on CPU. Training may take a while...\n')
  else:
    print('WARNING: Running on GPU. Ressources may be temporarly blocked...\n')

def get_logs():
  """Setup logging"""

  logdir = os.path.join('logs', datetime.datetime.now().strftime('%Y-%m-%d'))
  print('INFO: Logs will be written to', logdir, '\n')
  return tf.summary.create_file_writer(logdir)

def get_metrics():
  """Setup metrics"""

  metrics = { 
    'loss/generator' : keras.metrics.Mean(name='Loss:Generator'),
    'loss/discriminator' : keras.metrics.Mean(name='Loss:Discriminator'),
    'loss/realVSfake' : keras.metrics.Mean(name='Weight:Real/Fake'),
    'eval/kld' : keras.metrics.KLDivergence(name='Divergence:Kullbackâ€“Leibler'),
    'eval/mse' : keras.metrics.MeanSquaredError(name='Divergence:Mean-squared-Error')
  }
  return metrics

"""## Data

*   Choose dataset
*   Load dataset
*   Explore dataset
*   Preprocess dataset
*   Visualize dataset

### Parameters
"""

_BS = 64 #@param {type:"slider", min:32, max:128, step:32}
_DIM = 28 #@param ["28"] {type:"raw"}
_SHAPE = (_DIM, _DIM, 1) # Image shape
_SIZE = '[:100%]' # Dataset size

_DATA = {
    'Batch size' : _BS,
    'Image dimension' : _DIM,
    'Image shape' : _SHAPE
}

if _SHOWCASE:
  _SIZE = '[:10%]' # Decrease dataset size for showcase

"""### Choose dataset"""

dataset = 'mnist'

"""### Load dataset"""

def load_dataset(dataset):
  """Load dataset by means of TFDS (TensorFlow Datasets)
  
  Args:
    dataset: str
  
  Returns:
    train: tf.data.Dataset
    test: tf.data.Dataset
  """

  (train, test), info = tfds.load(dataset,
                            split=['train'+_SIZE, 'test'+_SIZE],
                            as_supervised=True,
                            with_info=True)

  print('Description:', info.description)
  print('Source:', info.homepage)
  print('Total number of examples:', info.splits['train'].num_examples + info.splits['test'].num_examples)
  return train, test

train_dataset, test_dataset = load_dataset(dataset)

"""### Explore dataset"""

def balance(titles, *argv):
  """Balance of datasets"""

  fig = plt.figure(figsize=(16, 8))

  for num, dataset in enumerate(argv):
    size = tf.data.experimental.cardinality(dataset)

    for image, label in dataset.batch(size).take(-1):
      y, idx, count = tf.unique_with_counts(label)
      fig.add_subplot(1, len(argv), num+1)
      plt.pie(count, autopct='%1.1f%%')
      plt.legend(labels=y.numpy())
      plt.title(titles[num] + ' label distribution')

  plt.show()

balance(['Train', 'Test'], train_dataset, test_dataset)

"""### Preprocess dataset"""

def normalize(image, label):
  """Normalize dataset 
  
  Normalize:
    Cast -> Normalize

  Args:
    image: tf.Tensor as Integer
    label: tf.Tensor as Integer
  
  Returns:
    image: tf.Tensor as Float
    label: tf.Tensor as Integer
  """

  image = tf.cast(image, tf.float32) # 0...255
  image = (image - 127.5) / 127.5 # -1...1
  return image, label

def preprocess(dataset, shuffle=True, batch=True, prefetch=True):
  """Preprocess dataset

  Preprocess: 
    Normalize -> Shuffle -> Batch -> Prefetch
  
  Args:
    dataset: tf.data.Dataset
    shuffle: boolean
    batch: boolean
  
  Returns:
    dataset: tf.data.Dataset
  """

  dataset = dataset.map(normalize)
  if shuffle: dataset = dataset.shuffle(tf.data.experimental.cardinality(dataset))
  if batch: dataset = dataset.batch(_BS, drop_remainder=True)
  if prefetch: dataset = dataset.prefetch(1)
  return dataset

train_dataset = preprocess(train_dataset)
test_dataset = preprocess(test_dataset)

"""### Visualize dataset"""

def visualize(dataset):
  """Visualize dataset
  
  Args:
    dataset: tf.data.Dataset
  """

  dataset = dataset.unbatch()
  fig = plt.figure(figsize=(16, 16))
  i = 0
  for image, label in dataset.take(36):
    fig.add_subplot(6, 6, i+1)
    plt.imshow(tf.squeeze(image, -1), cmap='gray')
    plt.title(label.numpy())
    plt.axis('off')
    i = i+1

  print('Real data instances:')
  plt.show()

visualize(train_dataset)

"""## Model

*   Choose model
*   Explore model
*   Compile model
*   Train model
*   Evaluate model

### Parameters
"""

_EPOCHS = 10 #@param {type:"slider", min:10, max:100, step:10}
_LR = 1e-3 # Learning rate
_DEPTH = 2 # Model depth

if _SHOWCASE:
  _EPOCHS = 4 # Decrease epochs for showcase

_MODEL = {
    'Epochs' : _EPOCHS,
    'Learning rate' : _LR
}

"""### Choose model

![GAN](./gan.png)

Fake data distribution $p_{noise}$

Real data distribution $p_{data}$

---

Generator $G$

Discriminator $D$

---

Noise $z \sim p_{noise}$

Original $x \sim p_{data}$

Copy $x' = G(z)$

Classification $c = D(x \lor x')$
"""

def get_generator(depth):
  """Generator as faking model
  
  Architecture: Encoder-Decoder
    Input -> Dense -> Reshape -> Convolution -> Normalization -> Activation ->
    Inverse Convolution
              
  Returns:
    keras.Model
  """

  first = keras.Input(shape=(196, ), name='Noise')
  layer = keras.layers.Dense(196)(first)
  layer = keras.layers.Reshape((7, 7, 4))(layer)

  for i in range(depth):
    layer = keras.layers.Conv2DTranspose(16*2**i, (3, 3), strides=2, padding='same')(layer)
    layer = keras.layers.BatchNormalization()(layer)
    layer = keras.layers.LeakyReLU()(layer)
  
  last = keras.layers.Conv2DTranspose(1, (3, 3), strides=1, padding='same', activation='tanh', name='Image')(layer)

  return keras.Model(inputs=first, outputs=last, name='Generator')

def generate(num):
  """Generate single image
  
  Architecture: Generator
    Noise -> Generator -> Image
              
  Returns:
    noise: Noise vector as generator input
    image: Image matrix as generator output
  """

  noise = tf.random.normal([num, 196]) # (num, 196)
  image = generator(noise, training=False) # (num, 28, 28, 1)
  image = tf.squeeze(image, 0) # (28, 28, 1)
  image = tf.squeeze(image, -1) # (28, 28)
  return noise, image

def get_discriminator(depth):
  """Discriminator as expertise model
  
  Architecture: Encoder
    Input -> Convolution -> Normalization -> Activation -> Flatten -> Dense
              
  Returns:
    keras.Model
  """

  first = keras.Input(shape=_SHAPE, name='Image')
  layer = first

  for i in reversed(range(depth)):
    layer = keras.layers.Conv2D(16*2**i, (3, 3), strides=2, padding='same')(layer)
    layer = keras.layers.BatchNormalization()(layer)
    layer = keras.layers.LeakyReLU()(layer)

  layer = keras.layers.Flatten()(layer)
  layer = keras.layers.Dense(98)(layer)
  last = keras.layers.Dense(1, name='Classification')(layer)

  return keras.Model(inputs=first, outputs=last, name='Discriminator')

generator = get_generator(_DEPTH)
discriminator = get_discriminator(_DEPTH)

"""### Explore model"""

generator.summary()

discriminator.summary()

def explore_model():
  """Explore model
  
  Pipeline: Generator -> Discriminator
    Noise -> Generator -> Image -> Discriminator -> Classification
  """

  # Generator
  noise, image = generate(1)
  fig = plt.figure(figsize=(16, 8))
  ax1 = fig.add_subplot(1, 2, 1)
  plt.hist(noise)
  ax1.title.set_text('Input (Noise)')
  ax2 = fig.add_subplot(1, 2, 2)
  plt.imshow(image, cmap='gray')
  plt.axis('off')
  ax2.title.set_text('Output (Image)')
  plt.suptitle('Generator')
  plt.show()

  # Discriminator
  classification = discriminator(tf.expand_dims(tf.expand_dims(image, 0), -1), training=False)
  fig = plt.figure(figsize=(16, 8))
  idx = tf.squeeze(classification, 0)
  ax1 = fig.add_subplot(1, 2, 1)
  plt.imshow(image, cmap='gray')
  plt.axis('off')
  ax1.title.set_text('Input (Image)')
  ax2 = fig.add_subplot(1, 2, 2)
  plt.plot([idx, idx], [0, 1], 'r-', label='Classification')
  plt.legend()
  ax2.title.set_text('Output (Classification)')
  ax2.axes.set_xticks([-1, 0, 1])
  ax2.axes.set_xticklabels(['Copy', 'Unsure', 'Original'])
  plt.suptitle('Discriminator')
  plt.show()

explore_model()

"""### Compile model

* Loss
   * Generator loss
   * Discriminator loss
* Optimizer
  * [Adam](https://arxiv.org/abs/1412.6980) (Adaptive Moment Estimation)
"""

def objective():
  """Binary crossentropy objective function
  
  Returns:
    keras.losses 
  """

  return keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
  """Generator loss
  
  Returns:
    generator_loss: tf.Tensor as Float
  """

  generator_loss = loss_fn(tf.ones_like(fake_output), fake_output)
  return generator_loss

def discriminator_loss(real_output, fake_output):
  """Discriminator loss
  
  Returns:
    discriminator_loss: tf.Tensor as Float
    loss_quotient: tf.Tensor as Float
  """

  real_loss = loss_fn(tf.ones_like(real_output), real_output)
  fake_loss = loss_fn(tf.zeros_like(fake_output), fake_output)
  discriminator_loss = real_loss + fake_loss
  loss_quotient = real_loss / fake_loss
  return discriminator_loss, loss_quotient

def optimizer(learning_rate):
  """Adam optimizer with exponential decay, as learning rate decreases over time
  
  Args:
    learning_rate: tf.float32
  
  Returns:
    keras.optimizer
  """
  
  schedule = keras.optimizers.schedules.ExponentialDecay(
      learning_rate,
      decay_steps=10,
      decay_rate=0.96)

  return tf.keras.optimizers.Adam(schedule)

loss_fn = objective()

"""### Train model"""

def train_step(images, noise, metrics):
  """Train step
  
  Step:
    Generate fake images -> Update evaluation metrics -> 
    Classify fake/real images -> Compute losses -> 
    Update loss metrics -> Compute gradients -> Apply gradients

  Args:
    images: tf.Tensor as Float
      Real data distribution
    noise: tf.Tensor as Float
      Fake data distribution
    metrics: Metrics in dict
  """

  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    
    generated_images = generator(noise, training=True)

    metrics['eval/kld'].update_state(images, generated_images)
    metrics['eval/mse'].update_state(images, generated_images)
    
    real_output = discriminator(images, training=True)
    fake_output = discriminator(generated_images, training=True)

    gen_loss = generator_loss(fake_output)
    metrics['loss/generator'].update_state(gen_loss)
    disc_loss, realVSfake = discriminator_loss(real_output, fake_output)
    metrics['loss/discriminator'].update_state(disc_loss)
    metrics['loss/realVSfake'].update_state(realVSfake)

  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

  optimizer(learning_rate=_LR).apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
  optimizer(learning_rate=_LR).apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train_model(dataset):
  """Train model
  
  Train:
    Iterate over epochs -> Iterate over batches -> Train step per batch ->
    Save (and reset) metrics

  Args:
    images: tf.data.Dataset
  """

  check_env()
  logging = get_logs()
  metrics = get_metrics()
  start = timer()

  if _SHOWCASE:
    print('INFO: Training in showcase mode with reduced dataset size and training epochs...\n')

  print('\t|Loss\t\t\t|Evaluation\t|Time')
  print('Epoch\t|GEN\tDIS\tRvsF\t|KLD\tMSE\t|ELA\tETA')

  for epoch in range(1, _EPOCHS+1): # Iterate over epochs

    epoch_start = timer()
    output = [epoch, _EPOCHS] 

    for batch in dataset: # Iterate over batches
      image_batch = batch[0]
      noise = tf.random.normal([_BS, 196])
      train_step(image_batch, noise, metrics)

    with logging.as_default(): # Save non-scalar metrics
      tf.summary.histogram('input/noise', noise[-1], step=epoch)
      tf.summary.image('output/digit', generator(tf.expand_dims(noise[-1], 0), training=False), step=epoch)
      tf.summary.scalar('optimizer/lr', optimizer(learning_rate=_LR).lr(epoch), step=epoch)

      for key, metric in metrics.items(): # Save scalar metrics
        tf.summary.scalar(key, metric.result(), step=epoch)
        output.append(metric.result().numpy())
        metric.reset_states()

    output.append((timer()-start)/60)
    output.append(((timer()-epoch_start) * (_EPOCHS-epoch))/60)
    print('%i/%i\t|%.3f\t%.3f\t%.3f\t|%.3f\t%.3f\t|%.1f m\t%.1f m' % tuple(output))

train_model(train_dataset)

"""### Evaluate model"""

"""## Deployment

*   Use model
*   Export model
*   Export metadata

### Parameters
"""

_NUM = 36 #@param {type:"slider", min:8, max:64, step:8}

_DEPLOYMENT = {
    'Number of digits' : _NUM
}

"""### Use model"""

def generate_multiple(num):
  """Generate multiple digits

  Args:
    num: Number of digits to generate
  """

  fig = plt.figure(figsize=(16, 16))

  for i in range(num):
    fig.add_subplot(math.sqrt(num), math.sqrt(num), i+1)
    _, image = generate(1)
    plt.imshow(image, cmap='gray')
    plt.axis('off')

  print('Fake data instances:')
  plt.show()

generate_multiple(_NUM)

"""### Export model"""

export = False #@param ["False", "True"] {type:"raw"}
if export: # Export model
  generator.save('./generator.h5')
  discriminator.save('./discriminator.h5')

plot = False #@param ["False", "True"] {type:"raw"}
if plot:
  keras.utils.plot_model(generator, to_file='generator.png', show_shapes=True)
  keras.utils.plot_model(discriminator, to_file='discriminator.png', show_shapes=True)

"""### Export metrics"""